models:
  clip:
    weight_path: '~/models/clip_finetuned/clip_finetune.pth'
    vocab_path: '~/models/clip_finetuned/bpe_simple_vocab_16e6.txt.gz'
  pstr:
    # Also edit data_root and data.test
    config_path: './configs/pstr/tdbaseline.py'
    weight_path: '~/models/pstr_resnet_cuhk/pstr_r50_cuhk.pth'

process:
  tokens_batch_size: 512
  crops_batch_size: 256
  frames_batch_size: 3
  num_workers: 2

eval:
  detection_reid:
    threshold: .25
    weight: .5

data:
  root_folder: '~/data/csu'
  frames_folder: '~/data/csu/frames'
  crops_folder: '~/data/csu/crops'

h5_files:
  captions_output: './outputs/crop_index_to_captions_output.h5'
  detection_output: './outputs/frame_file_to_detection_output.h5'
  crop_features_from_files: './outputs/crop_index_to_crop_features_from_files.h5'
  crop_features_from_annotations: './outputs/crop_index_to_crop_features_from_annotations.h5'
  crop_features_from_detections: './outputs/crop_index_to_crop_features_from_detections.h5'
  bboxes_clip_features: './outputs/frame_id_to_bboxes_clip_features.h5'
