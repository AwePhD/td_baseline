{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from tools.iou_utils import get_max_iou\n",
    "\n",
    "from cv2 import rectangle\n",
    "from mmcv import Config\n",
    "from mmcv.runner import wrap_fp16_model, load_checkpoint\n",
    "from mmdet.models import build_detector\n",
    "from mmcv.parallel import MMDataParallel\n",
    "from mmdet.datasets import (build_dataloader, build_dataset,\n",
    "                            replace_ImageToTensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PSTR's person search\n",
    "\n",
    "This notebook uses PSTR model to illustrate the person search step-by-step.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data, model and compute features.\n",
    "\n",
    "Those are the line of codes needed to run PSTR on some hand-picked samples for illustration.\n",
    "\n",
    "s9 s10 puis des autres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configs file with boiler plate\n",
    "cfg = Config.fromfile(Path(\"./configs/pstr/pstr_r50_24e_cuhk.py\"))\n",
    "cfg.model.pretrained = None\n",
    "if cfg.model.get('neck'):\n",
    "    if isinstance(cfg.model.neck, list):\n",
    "        for neck_cfg in cfg.model.neck:\n",
    "            if neck_cfg.get('rfp_backbone'):\n",
    "                if neck_cfg.rfp_backbone.get('pretrained'):\n",
    "                    neck_cfg.rfp_backbone.pretrained = None\n",
    "    elif cfg.model.neck.get('rfp_backbone'):\n",
    "        if cfg.model.neck.rfp_backbone.get('pretrained'):\n",
    "            cfg.model.neck.rfp_backbone.pretrained = None\n",
    "if isinstance(cfg.data.test, dict):\n",
    "    cfg.data.test.test_mode = True\n",
    "    samples_per_gpu = cfg.data.test.pop('samples_per_gpu', 1)\n",
    "    if samples_per_gpu > 1:\n",
    "        # Replace 'ImageToTensor' to 'DefaultFormatBundle'\n",
    "        cfg.data.test.pipeline = replace_ImageToTensor(\n",
    "            cfg.data.test.pipeline)\n",
    "elif isinstance(cfg.data.test, list):\n",
    "    for ds_cfg in cfg.data.test:\n",
    "        ds_cfg.test_mode = True\n",
    "    samples_per_gpu = max(\n",
    "        [ds_cfg.pop('samples_per_gpu', 1) for ds_cfg in cfg.data.test])\n",
    "    if samples_per_gpu > 1:\n",
    "        for ds_cfg in cfg.data.test:\n",
    "            ds_cfg.pipeline = replace_ImageToTensor(ds_cfg.pipeline)\n",
    "cfg.model.train_cfg = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.22s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# Get pedes_dataset\n",
    "pedes_dataset = build_dataset(cfg.data.test)\n",
    "data_loader = build_dataloader(\n",
    "    [ pedes_dataset[i] for i in range(100+1)],\n",
    "    samples_per_gpu=1,\n",
    "    workers_per_gpu=1,\n",
    "    dist=False,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = next(iter(data_loader))\n",
    "b_img, b_img_metas = (b['img'], b['img_metas'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1, torch.Size([1, 3, 844, 1500]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b_img), len(b_img), b_img[0].shape, "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## img_metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DataContainer([[{'filename': '/sysu/Image/SSM/s15535.jpg', 'ori_filename': 's15535.jpg', 'ori_shape': (450, 800, 3), 'img_shape': (844, 1500, 3), 'pad_shape': (844, 1500, 3), 'scale_factor': array([1.875    , 1.8755555, 1.875    , 1.8755555], dtype=float32), 'flip': False, 'flip_direction': None, 'img_norm_cfg': {'mean': array([123.675, 116.28 , 103.53 ], dtype=float32), 'std': array([58.395, 57.12 , 57.375], dtype=float32), 'to_rgb': True}}]])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_img_metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, mmcv.parallel.data_container.DataContainer, list, 1, list, 1, dict)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    len(b_img_metas), \n",
    "    type(b_img_metas[0]), \n",
    "    type(b_img_metas[0].data), \n",
    "    len(b_img_metas[0].data),\n",
    "    type(b_img_metas[0].data[0]),\n",
    "    len(b_img_metas[0].data[0]),\n",
    "    type(b_img_metas[0].data[0][0]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filename': '/sysu/Image/SSM/s15535.jpg',\n",
       " 'ori_filename': 's15535.jpg',\n",
       " 'ori_shape': (450, 800, 3),\n",
       " 'img_shape': (844, 1500, 3),\n",
       " 'pad_shape': (844, 1500, 3),\n",
       " 'scale_factor': array([1.875    , 1.8755555, 1.875    , 1.8755555], dtype=float32),\n",
       " 'flip': False,\n",
       " 'flip_direction': None,\n",
       " 'img_norm_cfg': {'mean': array([123.675, 116.28 , 103.53 ], dtype=float32),\n",
       "  'std': array([58.395, 57.12 , 57.375], dtype=float32),\n",
       "  'to_rgb': True}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_img_metas[0].data[0][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
